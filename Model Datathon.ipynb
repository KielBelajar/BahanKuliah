{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d237d577",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10019cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\keaga\\anaconda3\\lib\\site-packages (4.52.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\keaga\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\keaga\\anaconda3\\lib\\site-packages (from transformers) (0.33.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\keaga\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\keaga\\anaconda3\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\keaga\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\keaga\\anaconda3\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\keaga\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\keaga\\anaconda3\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\keaga\\anaconda3\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\keaga\\anaconda3\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\keaga\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\keaga\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\keaga\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\keaga\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\keaga\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\keaga\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\keaga\\anaconda3\\lib\\site-packages (from requests->transformers) (2025.4.26)\n",
      "Requirement already satisfied: Sastrawi in c:\\users\\keaga\\anaconda3\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: langdetect in c:\\users\\keaga\\anaconda3\\lib\\site-packages (1.0.9)\n",
      "Requirement already satisfied: six in c:\\users\\keaga\\anaconda3\\lib\\site-packages (from langdetect) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install Sastrawi\n",
    "!pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "968ff6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library untuk memuat dan menganalisis data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# import library untuk text cleaning\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "import re\n",
    "\n",
    "# import library untuk deteksi bahasa\n",
    "from langdetect import detect, LangDetectException\n",
    "\n",
    "# import pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# import library untuk membuat model\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import pipeline\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# import library untuk menyimpan model yang sudah dibuat\n",
    "import joblib "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a30eed2",
   "metadata": {},
   "source": [
    "# Memuat Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72cd3c46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nama Pengguna</th>\n",
       "      <th>Teks Ulasan</th>\n",
       "      <th>Rating Bintang</th>\n",
       "      <th>Tanggal Ulasan</th>\n",
       "      <th>Jumlah Likes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pengguna Google</td>\n",
       "      <td>pelit pengen karakter aja susah malah dapet yg...</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-06-20 14:40:33</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pengguna Google</td>\n",
       "      <td>farming artefak engga pernah dikasih stat/sub ...</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-06-20 14:00:33</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pengguna Google</td>\n",
       "      <td>udah ngga worth it, perbaiki diri mu genshin, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-06-20 13:49:19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pengguna Google</td>\n",
       "      <td>kasih fitur skip.</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-06-20 13:22:21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pengguna Google</td>\n",
       "      <td>peak update</td>\n",
       "      <td>5</td>\n",
       "      <td>2025-06-20 13:16:04</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Nama Pengguna                                        Teks Ulasan  \\\n",
       "0  Pengguna Google  pelit pengen karakter aja susah malah dapet yg...   \n",
       "1  Pengguna Google  farming artefak engga pernah dikasih stat/sub ...   \n",
       "2  Pengguna Google  udah ngga worth it, perbaiki diri mu genshin, ...   \n",
       "3  Pengguna Google                                  kasih fitur skip.   \n",
       "4  Pengguna Google                                        peak update   \n",
       "\n",
       "   Rating Bintang       Tanggal Ulasan  Jumlah Likes  \n",
       "0               1  2025-06-20 14:40:33             0  \n",
       "1               1  2025-06-20 14:00:33             0  \n",
       "2               1  2025-06-20 13:49:19             0  \n",
       "3               1  2025-06-20 13:22:21             0  \n",
       "4               5  2025-06-20 13:16:04             0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data = \"https://github.com/keaganwjy/datathon/raw/refs/heads/main/reviews_genshin_impact_raw.csv\"\n",
    "jumlah_baris = 2000 # jumlah baris yang akan dimasukan.as_integer_ratio\n",
    "df_raw = pd.read_csv(Data) # membaca CSV\n",
    "df = df_raw.head(jumlah_baris).copy() # menggunakan .head() untuk mengambil baris teratas secara langsung\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb77fc0f",
   "metadata": {},
   "source": [
    "# Data Cleaning Dengan Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75db7a57",
   "metadata": {},
   "source": [
    "1. Penyesuaian Data\n",
    "2. Pembersihan Text\n",
    "3. Deteksi Bahasa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc82d39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class untuk menyesuaikan tipe data\n",
    "class PenyesuaianTipeData(BaseEstimator,TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # mengubah 'Tanggal Ulasan' menjadi format datetime\n",
    "        # ini memungkinkan kita untuk melakukan analisis berbasis waktu nanti\n",
    "        X['Tanggal Ulasan'] = pd.to_datetime(X['Tanggal Ulasan'])\n",
    "        \n",
    "        # memastikan 'Rating Bintang' dan 'Jumlah Likes' adalah angka (integer)\n",
    "        # error='coerce' akan mengubah nilai yang tidak bisa diubah menjadi angka (misal: teks) menjadi NaN (kosong)\n",
    "        X['Rating Bintang'] = pd.to_numeric(X['Rating Bintang'], errors='coerce')\n",
    "        X['Jumlah Likes'] = pd.to_numeric(X['Jumlah Likes'], errors='coerce')\n",
    "        \n",
    "        # kita bisa membuang baris yang rating atau likes-nya menjadi NaN, atau mengisinya dengan 0\n",
    "        X.dropna(subset=['Rating Bintang'], inplace=True) # rating wajib ada\n",
    "        # kode baru (praktik terbaik dan aman)\n",
    "        X['Jumlah Likes'] = X['Jumlah Likes'].fillna(0)\n",
    "        \n",
    "        # mengubah tipe data menjadi integer untuk efisiensi memori\n",
    "        X['Rating Bintang'] = X['Rating Bintang'].astype(int)\n",
    "        X['Jumlah Likes'] = X['Jumlah Likes'].astype(int)\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4ad4e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class untuk pembersihan teks\n",
    "class PembersihanText(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # inisialisasi stemmer dan stopword remover\n",
    "        factory_stemmer = StemmerFactory()\n",
    "        stemmer = factory_stemmer.create_stemmer()\n",
    "        factory_stopword = StopWordRemoverFactory()\n",
    "        stopword_remover = factory_stopword.create_stop_word_remover()\n",
    "        \n",
    "        # buat kamus slang yang lebih lengkap (ini bisa terus Anda kembangkan)\n",
    "        slang_dict = {\n",
    "        'bgt': 'banget', 'gak': 'tidak', 'ga': 'tidak', 'kalo': 'kalau', 'gacha': 'gacha',\n",
    "        'dgn': 'dengan', 'krn': 'karena', 'yg': 'yang', 'utk': 'untuk', 'mantap': 'mantap',\n",
    "        'keren': 'keren', 'bug': 'bug', 'ngebug': 'bug', 'loding': 'loading', 'ngelek': 'lag',\n",
    "        'ngeframe': 'frame', 'drop': 'drop', 'jelek': 'jelek', 'bangettt': 'banget',\n",
    "        'sih': '', 'nya': '', 'aja': 'saja', 'kok': '', 'sih': ''\n",
    "        }\n",
    "        \n",
    "        def preprocess_text(text):\n",
    "            # pastikan input adalah string\n",
    "            if not isinstance(text, str):\n",
    "                return \"\"\n",
    "            # 1. case folding\n",
    "            text = text.lower()\n",
    "            # 2. hapus noise (URL, mention, hashtag, karakter non-alfabet)\n",
    "            text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "            text = re.sub(r'\\@\\w+|\\#', '', text)\n",
    "            text = re.sub(r'[^a-z\\s]', '', text)\n",
    "            # 3. normalisasi kata slang\n",
    "            words = text.split()\n",
    "            normalized_words = [slang_dict[word] if word in slang_dict else word for word in words]\n",
    "            text = \" \".join(normalized_words)\n",
    "            # 4. Stopword Removal\n",
    "            text = stopword_remover.remove(text)\n",
    "            # 5. stemming\n",
    "            text = stemmer.stem(text)\n",
    "            return text\n",
    "        \n",
    "        # menggunakan .copy() untuk menghindari SettingWithCopyWarning\n",
    "        X_processed = X.copy()\n",
    "        X_processed['Ulasan Bersih'] = X_processed['Teks Ulasan'].apply(preprocess_text)\n",
    "        return X_processed\n",
    "                \n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64b1c383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class untuk  medeteksi bahasa \n",
    "class DeteksiBahasa(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        kolom_krusial = ['Ulasan Bersih', 'Rating Bintang', 'Tanggal Ulasan']\n",
    "        X.dropna(subset=kolom_krusial, inplace=True)\n",
    "        \n",
    "        # cek jumlah baris setelah pembersihan untuk melihat perbedaannya\n",
    "        X.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        # 1. Definisikan fungsi yang aman (pastikan nama ini yanga digunakan)\n",
    "        def deteksi_bahasa_aman(teks):\n",
    "            # cek apakah inputnya bukan string (misalnya NaN)\n",
    "            if pd.isna(teks) or not isinstance(teks, str):\n",
    "                return 'undefined'\n",
    "            \n",
    "            # cek apakah teks terlalu pendek\n",
    "            if len(teks.strip()) < 10:\n",
    "                return 'pendek'\n",
    "            \n",
    "            # coba deteksi bahasa\n",
    "            try:\n",
    "                return detect(teks)\n",
    "            except LangDetectException:\n",
    "                return 'error'\n",
    "        \n",
    "    # 2. Terapkan fungsi yang benar pada dataframe anda\n",
    "    # asumsikan 'X' adalah dataframe anda\n",
    "    # perbaikan di sini: Panggil 'deteksi_bahasa_aman'\n",
    "        X['bahasa'] = X['Ulasan Bersih'].apply(deteksi_bahasa_aman)\n",
    "        \n",
    "        # hasil\n",
    "        print(\"\\nDistribusi Bahasa pada Dataset\")\n",
    "        print(X['bahasa'].value_counts())\n",
    "        return X\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af0922e",
   "metadata": {},
   "source": [
    "# Labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aa3cd63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Labelling(BaseEstimator, TransformerMixin):\n",
    "            \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self,X):\n",
    "        os.environ[\"HF_HUB_DOWNLOAD_TIMEOUT\"] = \"60\"  # Timeout jadi 60 detik\n",
    "        classifier = pipeline(\n",
    "            \"zero-shot-classification\",\n",
    "            model=\"cahya/distilbert-base-indonesian\",\n",
    "            device=0  # atau -1 jika tidak ada GPU\n",
    "        )\n",
    "        \n",
    "        print(\"Model berhasil dimuat.\")\n",
    "\n",
    "        # --- Definisikan Fungsi Pelabelan ---\n",
    "        # Daftar kategori final Anda\n",
    "        candidate_labels = ['Cerita', 'Gameplay', 'Grafis', 'Bugs & Error', 'Optimalisasi', 'Monetisasi & Gacha', 'Komunitas'] # Tambah/ubah sesuai kebutuhan\n",
    "\n",
    "        def get_auto_labels(text):\n",
    "            # Ambang batas skor agar sebuah label dianggap relevan\n",
    "            threshold = 0.60 # Anda bisa menyesuaikan ini nanti\n",
    "\n",
    "            try:\n",
    "                result = classifier(text, candidate_labels, multi_label=True)\n",
    "\n",
    "                # Ambil label yang skornya di atas ambang batas\n",
    "                labels = [label for label, score in zip(result['labels'], result['scores']) if score > threshold]\n",
    "\n",
    "                # Jika tidak ada label di atas ambang batas, kembalikan list kosong\n",
    "                return labels if labels else []\n",
    "            except Exception as e:\n",
    "                # Jika terjadi error pada teks tertentu\n",
    "                print(f\"Error pada teks: {text[:50]}... | Error: {e}\")\n",
    "                return [\"error\"]\n",
    "\n",
    "        # --- Terapkan pada Sampel Kecil untuk Uji Coba ---\n",
    "        # PENTING: Jalankan pada sampel kecil dulu (misal: 1000-2000 baris)\n",
    "        # untuk menghemat waktu dan memvalidasi proses.\n",
    "        X_sample = X.head(2000).copy()\n",
    "        X_sample = X_sample[X_sample['Ulasan Bersih'].notna() & (X_sample['Ulasan Bersih'].str.strip() != \"\")]\n",
    "\n",
    "        X_sample['auto_labels'] = [\n",
    "            classifier(text, candidate_labels, multi_label=True)\n",
    "            for text in tqdm(X_sample['Ulasan Bersih'])\n",
    "        ]\n",
    "        print(\"Pelabelan otomatis selesai.\")\n",
    "\n",
    "        # Mari kita lihat hasilnya. Hasilnya akan sedikit berbeda, berupa dictionary.\n",
    "        # Kita akan ekstrak labelnya saja.\n",
    "        def extract_labels_from_result(result, threshold=0.60):\n",
    "            return [label for label, score in zip(result['labels'], result['scores']) if score > threshold]\n",
    "\n",
    "        X_sample['auto_labels_list'] = [extract_labels_from_result(res) for res in X_sample['auto_labels']]\n",
    "\n",
    "\n",
    "        # Tampilkan kolom-kolom yang relevan\n",
    "        print(\"\\nContoh Hasil Pelabelan Otomatis:\")\n",
    "        print(X_sample[['Ulasan Bersih', 'auto_labels_list']].head(10))\n",
    "        \n",
    "        return X_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c8f553",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "760cb071",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Modelling(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Gunakan GPU jika tersedia\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Load IndoBERT\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"indobenchmark/indobert-base-p1\")\n",
    "        model = AutoModel.from_pretrained(\"indobenchmark/indobert-base-p1\").to(device)\n",
    "        model.eval()\n",
    "\n",
    "        # buat label sentimen manual (bisa juga diganti dengan anotasi jika tersedia)\n",
    "        def simple_sentiment(text):\n",
    "            text = text.lower()\n",
    "            if any(k in text for k in ['jelek', 'buruk', 'crash', 'lag', 'sampah']):\n",
    "                return \"negatif\"\n",
    "            elif any(k in text for k in ['bagus', 'keren', 'mantap', 'seru']):\n",
    "                return \"positif\"\n",
    "            else:\n",
    "                return \"netral\"\n",
    "            \n",
    "        # terapkan label sentimen\n",
    "        X['sentimen'] = X['Ulasan Bersih'].apply(simple_sentiment)\n",
    "        \n",
    "        # encode label sentimen\n",
    "        label_encoder = LabelEncoder()\n",
    "        X['label_sentimen'] = label_encoder.fit_transform(X['sentimen']) # 0 = negatif, 1 = netral, 2 = positif\n",
    "        \n",
    "        # melakukan feature extractor untuk IndoBERT\n",
    "        def get_bert_embedding(text, max_len=128):\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding='max_length', max_length=max_len)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "            # Ambil CLS token sebagai representasi kalimat\n",
    "            cls_embedding = outputs.last_hidden_state[:, 0, :].squeeze().cpu().numpy()\n",
    "            return cls_embedding\n",
    "        \n",
    "        # ambil embedding untuk seluruh data\n",
    "        print(\"Mengekstrak embedding IndoBERT...\")\n",
    "        embeddings = np.array([get_bert_embedding(text) for text in tqdm(X['Ulasan Bersih'])])\n",
    "        \n",
    "        # latih model machine learning\n",
    "        X_mod = embeddings\n",
    "        y_mod = label_encoder.fit_transform(X['sentimen']) \n",
    "        \n",
    "        X_train_mod, X_test_mod, y_train_mod, y_test_mod = train_test_split(X_mod, y_mod, test_size=0.2, random_state = 42)\n",
    "        \n",
    "        # latih model dengan Logistic Regression \n",
    "        clf = LogisticRegression(max_iter=1000)\n",
    "        clf.fit(X_train_mod, y_train_mod)\n",
    "        \n",
    "        y_pred = clf.predict(X_test_mod)\n",
    "        \n",
    "        # Evaluasi\n",
    "        print(\"\\nHasil evaluasi model sentimen\")\n",
    "        print(classification_report(y_test_mod, y_pred, target_names=label_encoder.classes_))\n",
    "        \n",
    "        # prediksi dan kategorisasi game\n",
    "        # terapkan prediksi ke seluruh data\n",
    "        X['pred_sentimen'] = label_encoder.inverse_transform(clf.predict(X_mod))\n",
    "        \n",
    "        # Buat ringkasan per kategori berdasarkan label zero-shot sebelumnya\n",
    "        from collections import Counter\n",
    "        \n",
    "        def ringkasan_kategori(X):\n",
    "            hasil = []\n",
    "            candidate = ['Cerita', 'Gameplay', 'Grafis', 'Bugs & Error', \n",
    "                        'Optimalisasi', 'Monetisasi & Gacha', 'Komunitas']\n",
    "            for kategori in candidate:\n",
    "                subset = X[X['auto_labels_list'].apply(lambda x: kategori in x)]\n",
    "                total = len(subset)\n",
    "                if total == 0:\n",
    "                    continue\n",
    "                counter = Counter(subset['pred_sentimen'])\n",
    "                ringkas = {\n",
    "                    'Kategori': kategori,\n",
    "                    'Total Ulasan': total,\n",
    "                    'Positif': counter['positif'],\n",
    "                    'Negatif': counter['negatif'],\n",
    "                    'Netral': counter['netral'],\n",
    "                }\n",
    "                hasil.append(ringkas)\n",
    "            return pd.DataFrame(hasil)\n",
    "        \n",
    "        df_summary = ringkasan_kategori(X)\n",
    "        print(\"\\nRingkasan Sentimen per Kategori:\")\n",
    "        print(df_summary)\n",
    "\n",
    "        # prediksi tingkat akurasi model\n",
    "        accuracy = accuracy_score(y_test_mod,y_pred)\n",
    "        print(f\"\\nTingkat akurasi model: {accuracy}\")\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5638158",
   "metadata": {},
   "source": [
    "# Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "438c8b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "Pipe = Pipeline([\n",
    "    (\"Penyesuaian\",PenyesuaianTipeData()),\n",
    "    (\"Pembersihan\",PembersihanText()),\n",
    "    (\"Deteksi\",DeteksiBahasa()),\n",
    "    (\"Labelling\",Labelling()),\n",
    "    (\"Model\", Modelling())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e3f347c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distribusi Bahasa pada Dataset\n",
      "bahasa\n",
      "id        1182\n",
      "pendek     308\n",
      "tl         147\n",
      "en         104\n",
      "lt          43\n",
      "no          21\n",
      "so          19\n",
      "sw          18\n",
      "sq          16\n",
      "nl          14\n",
      "da          14\n",
      "sl          13\n",
      "et          12\n",
      "af          11\n",
      "ca          10\n",
      "ro          10\n",
      "hr           9\n",
      "tr           9\n",
      "fi           8\n",
      "it           8\n",
      "cy           7\n",
      "es           3\n",
      "fr           3\n",
      "hu           2\n",
      "de           2\n",
      "sk           2\n",
      "pt           2\n",
      "sv           1\n",
      "pl           1\n",
      "lv           1\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at cahya/distilbert-base-indonesian and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n",
      "Failed to determine 'entailment' label id from the label2id mapping in the model config. Setting to -1. Define a descriptive label2id mapping in the model config to ensure correct outputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model berhasil dimuat.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1979 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "100%|██████████| 1979/1979 [05:08<00:00,  6.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pelabelan otomatis selesai.\n",
      "\n",
      "Contoh Hasil Pelabelan Otomatis:\n",
      "                                       Ulasan Bersih auto_labels_list\n",
      "0          pelit ken karakter susah malah dapet lain               []\n",
      "1  farming artefak engga pernah kasih statsub sta...               []\n",
      "2  udah ngga worth it baik diri mu genshin dengar...               []\n",
      "3                                   kasih fitur skip               []\n",
      "4                                        peak update               []\n",
      "5                                               seru               []\n",
      "6  game ada kembang sama sekali scene kaku desain...               []\n",
      "7  kuota ku habis cuman mendondlod data beri pili...               []\n",
      "8                               bagus banget gamenya               []\n",
      "9      game e apik poll lek hp ne kentang ojok maksa               []\n",
      "Mengekstrak embedding IndoBERT...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1979/1979 [02:32<00:00, 13.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hasil evaluasi model sentimen\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     negatif       0.54      0.32      0.40        22\n",
      "      netral       0.94      0.93      0.93       246\n",
      "     positif       0.85      0.93      0.89       128\n",
      "\n",
      "    accuracy                           0.89       396\n",
      "   macro avg       0.78      0.72      0.74       396\n",
      "weighted avg       0.89      0.89      0.89       396\n",
      "\n",
      "\n",
      "Ringkasan Sentimen per Kategori:\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n",
      "\n",
      "Tingkat akurasi model: 0.8939393939393939\n"
     ]
    }
   ],
   "source": [
    "model = Pipe.transform(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
